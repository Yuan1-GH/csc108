"""
H5è½¬CSVå¤„ç†å™¨
åŠŸèƒ½ï¼šå°†H5æ–‡ä»¶è½¬æ¢ä¸ºCSVæ ¼å¼ï¼Œç”ŸæˆåŸå§‹ç‰ˆæœ¬å’Œæ¸…æ´—åç‰ˆæœ¬ä¸¤ä¸ªæ–‡ä»¶
"""

import pandas as pd
import numpy as np
import os
from typing import Optional, Tuple, Dict, Any
import warnings
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class H5ToCSVProcessor:
    """H5è½¬CSVå¤„ç†å™¨ï¼Œç”ŸæˆåŸå§‹å’Œæ¸…æ´—åçš„CSVæ–‡ä»¶"""
    
    def __init__(self):
        self.raw_data = None
        self.cleaned_data = None
        self.file_path = None
        
    def load_h5_data(self, file_path: str) -> bool:
        """
        åŠ è½½H5æ–‡ä»¶æ•°æ®
        
        Args:
            file_path: H5æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            if not os.path.exists(file_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            logger.info(f"æ­£åœ¨åŠ è½½H5æ–‡ä»¶: {file_path}")
            
            # å°è¯•è¯»å–H5æ–‡ä»¶
            self.raw_data = pd.read_hdf(file_path)
            self.file_path = file_path
            
            logger.info(f"æˆåŠŸåŠ è½½H5æ•°æ®ï¼Œå½¢çŠ¶: {self.raw_data.shape}")
            logger.info(f"åˆ—å: {list(self.raw_data.columns)}")
            
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½H5æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def analyze_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†ææ•°æ®è´¨é‡
        
        Args:
            data: è¦åˆ†æçš„æ•°æ®
            
        Returns:
            Dict: æ•°æ®è´¨é‡æŠ¥å‘Š
        """
        report = {
            "åŸºæœ¬ä¿¡æ¯": {
                "æ•°æ®å½¢çŠ¶": data.shape,
                "åˆ—æ•°": len(data.columns),
                "è¡Œæ•°": len(data),
                "å†…å­˜ä½¿ç”¨": f"{data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB"
            },
            "ç¼ºå¤±å€¼ç»Ÿè®¡": {},
            "æ•°æ®ç±»å‹": {},
            "å¼‚å¸¸å€¼ç»Ÿè®¡": {}
        }
        
        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing_stats = data.isnull().sum()
        report["ç¼ºå¤±å€¼ç»Ÿè®¡"] = {
            col: {
                "ç¼ºå¤±æ•°é‡": int(missing_stats[col]),
                "ç¼ºå¤±æ¯”ä¾‹": f"{missing_stats[col] / len(data) * 100:.2f}%"
            }
            for col in data.columns if missing_stats[col] > 0
        }
        
        # æ•°æ®ç±»å‹
        report["æ•°æ®ç±»å‹"] = {col: str(dtype) for col, dtype in data.dtypes.items()}
        
        # æ•°å€¼åˆ—çš„å¼‚å¸¸å€¼ç»Ÿè®¡
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if len(data[col].dropna()) > 0:
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()
                if outliers > 0:
                    report["å¼‚å¸¸å€¼ç»Ÿè®¡"][col] = {
                        "å¼‚å¸¸å€¼æ•°é‡": int(outliers),
                        "å¼‚å¸¸å€¼æ¯”ä¾‹": f"{outliers / len(data) * 100:.2f}%",
                        "ä¸‹ç•Œ": lower_bound,
                        "ä¸Šç•Œ": upper_bound
                    }
        
        return report
    
    def clean_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°æ®æ¸…æ´—
        
        Args:
            data: åŸå§‹æ•°æ®
            
        Returns:
            pd.DataFrame: æ¸…æ´—åçš„æ•°æ®
        """
        logger.info("å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        cleaned_data = data.copy()
        cleaning_log = []
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        numeric_columns = cleaned_data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            missing_count = cleaned_data[col].isnull().sum()
            if missing_count > 0:
                # ä½¿ç”¨å‰å‘å¡«å……å’Œåå‘å¡«å……
                cleaned_data[col] = cleaned_data[col].fillna(method='ffill').fillna(method='bfill')
                
                # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œç”¨å‡å€¼å¡«å……
                if cleaned_data[col].isnull().sum() > 0:
                    mean_value = cleaned_data[col].mean()
                    cleaned_data[col] = cleaned_data[col].fillna(mean_value)
                
                cleaning_log.append(f"å¤„ç†åˆ— {col} çš„ {missing_count} ä¸ªç¼ºå¤±å€¼")
        
        # å¤„ç†éæ•°å€¼åˆ—çš„ç¼ºå¤±å€¼
        non_numeric_columns = cleaned_data.select_dtypes(exclude=[np.number]).columns
        for col in non_numeric_columns:
            missing_count = cleaned_data[col].isnull().sum()
            if missing_count > 0:
                # ç”¨å‰å‘å¡«å……å¤„ç†
                cleaned_data[col] = cleaned_data[col].fillna(method='ffill').fillna('Unknown')
                cleaning_log.append(f"å¤„ç†åˆ— {col} çš„ {missing_count} ä¸ªç¼ºå¤±å€¼")
        
        # 2. å¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
        for col in numeric_columns:
            if len(cleaned_data[col].dropna()) > 0:
                Q1 = cleaned_data[col].quantile(0.25)
                Q3 = cleaned_data[col].quantile(0.75)
                IQR = Q3 - Q1
                
                if IQR > 0:  # é¿å…é™¤é›¶é”™è¯¯
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    # ç»Ÿè®¡å¼‚å¸¸å€¼æ•°é‡
                    outliers_mask = (cleaned_data[col] < lower_bound) | (cleaned_data[col] > upper_bound)
                    outliers_count = outliers_mask.sum()
                    
                    if outliers_count > 0:
                        # ç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼
                        cleaned_data.loc[cleaned_data[col] < lower_bound, col] = lower_bound
                        cleaned_data.loc[cleaned_data[col] > upper_bound, col] = upper_bound
                        cleaning_log.append(f"å¤„ç†åˆ— {col} çš„ {outliers_count} ä¸ªå¼‚å¸¸å€¼")
        
        # 3. å»é™¤é‡å¤è¡Œ
        duplicate_count = cleaned_data.duplicated().sum()
        if duplicate_count > 0:
            cleaned_data = cleaned_data.drop_duplicates()
            cleaning_log.append(f"åˆ é™¤ {duplicate_count} ä¸ªé‡å¤è¡Œ")
        
        # 4. æ•°æ®ç±»å‹ä¼˜åŒ–
        for col in numeric_columns:
            if cleaned_data[col].dtype == 'float64':
                # å°è¯•è½¬æ¢ä¸ºfloat32ä»¥èŠ‚çœå†…å­˜
                if cleaned_data[col].min() >= np.finfo(np.float32).min and cleaned_data[col].max() <= np.finfo(np.float32).max:
                    cleaned_data[col] = cleaned_data[col].astype('float32')
                    cleaning_log.append(f"ä¼˜åŒ–åˆ— {col} æ•°æ®ç±»å‹ä¸º float32")
        
        logger.info("æ•°æ®æ¸…æ´—å®Œæˆ")
        for log in cleaning_log:
            logger.info(f"  - {log}")
        
        return cleaned_data
    
    def save_to_csv(self, data: pd.DataFrame, filename: str, description: str = "") -> bool:
        """
        ä¿å­˜æ•°æ®ä¸ºCSVæ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶å
            description: æ–‡ä»¶æè¿°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸä¿å­˜
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
            
            # ä¿å­˜CSVæ–‡ä»¶
            data.to_csv(filename, index=False, encoding='utf-8-sig')
            
            file_size = os.path.getsize(filename) / 1024 / 1024  # MB
            logger.info(f"æˆåŠŸä¿å­˜{description}: {filename}")
            logger.info(f"  - æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            logger.info(f"  - æ•°æ®å½¢çŠ¶: {data.shape}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜{description}å¤±è´¥: {e}")
            return False
    
    def generate_summary_report(self, original_data: pd.DataFrame, cleaned_data: pd.DataFrame) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤„ç†æ€»ç»“æŠ¥å‘Š
        
        Args:
            original_data: åŸå§‹æ•°æ®
            cleaned_data: æ¸…æ´—åæ•°æ®
            
        Returns:
            Dict: æ€»ç»“æŠ¥å‘Š
        """
        report = {
            "å¤„ç†æ—¶é—´": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æºæ–‡ä»¶": self.file_path,
            "åŸå§‹æ•°æ®": {
                "å½¢çŠ¶": original_data.shape,
                "ç¼ºå¤±å€¼æ€»æ•°": int(original_data.isnull().sum().sum()),
                "é‡å¤è¡Œæ•°": int(original_data.duplicated().sum())
            },
            "æ¸…æ´—åæ•°æ®": {
                "å½¢çŠ¶": cleaned_data.shape,
                "ç¼ºå¤±å€¼æ€»æ•°": int(cleaned_data.isnull().sum().sum()),
                "é‡å¤è¡Œæ•°": int(cleaned_data.duplicated().sum())
            },
            "æ¸…æ´—æ•ˆæœ": {
                "åˆ é™¤çš„ç¼ºå¤±å€¼": int(original_data.isnull().sum().sum() - cleaned_data.isnull().sum().sum()),
                "åˆ é™¤çš„é‡å¤è¡Œ": int(original_data.duplicated().sum() - cleaned_data.duplicated().sum()),
                "æ•°æ®ä¿ç•™ç‡": f"{len(cleaned_data) / len(original_data) * 100:.2f}%"
            }
        }
        
        return report
    
    def process_h5_to_csv(self, h5_file_path: str, output_dir: str = "output") -> bool:
        """
        å®Œæ•´çš„H5è½¬CSVå¤„ç†æµç¨‹
        
        Args:
            h5_file_path: H5æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå¤„ç†
        """
        try:
            # 1. åŠ è½½H5æ•°æ®
            if not self.load_h5_data(h5_file_path):
                return False
            
            # 2. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = Path(h5_file_path).stem
            os.makedirs(output_dir, exist_ok=True)
            
            original_csv = os.path.join(output_dir, f"{base_name}_original.csv")
            cleaned_csv = os.path.join(output_dir, f"{base_name}_cleaned.csv")
            
            # 3. åˆ†æåŸå§‹æ•°æ®è´¨é‡
            logger.info("åˆ†æåŸå§‹æ•°æ®è´¨é‡...")
            original_quality = self.analyze_data_quality(self.raw_data)
            
            # 4. ä¿å­˜åŸå§‹CSV
            logger.info("ä¿å­˜åŸå§‹CSVæ–‡ä»¶...")
            if not self.save_to_csv(self.raw_data, original_csv, "åŸå§‹æ•°æ®CSV"):
                return False
            
            # 5. æ•°æ®æ¸…æ´—
            logger.info("æ‰§è¡Œæ•°æ®æ¸…æ´—...")
            self.cleaned_data = self.clean_data(self.raw_data)
            
            # 6. ä¿å­˜æ¸…æ´—åçš„CSV
            logger.info("ä¿å­˜æ¸…æ´—åCSVæ–‡ä»¶...")
            if not self.save_to_csv(self.cleaned_data, cleaned_csv, "æ¸…æ´—åæ•°æ®CSV"):
                return False
            
            # 7. åˆ†ææ¸…æ´—åæ•°æ®è´¨é‡
            logger.info("åˆ†ææ¸…æ´—åæ•°æ®è´¨é‡...")
            cleaned_quality = self.analyze_data_quality(self.cleaned_data)
            
            # 8. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            summary_report = self.generate_summary_report(self.raw_data, self.cleaned_data)
            
            # 9. ä¿å­˜æŠ¥å‘Š
            report_file = os.path.join(output_dir, f"{base_name}_processing_report.json")
            full_report = {
                "summary": summary_report,
                "original_data_quality": original_quality,
                "cleaned_data_quality": cleaned_quality
            }
            
            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(full_report, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # 10. æ‰“å°æ€»ç»“
            print("\n" + "="*60)
            print("H5è½¬CSVå¤„ç†å®Œæˆï¼")
            print("="*60)
            print(f"æºæ–‡ä»¶: {h5_file_path}")
            print(f"è¾“å‡ºç›®å½•: {output_dir}")
            print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"  ğŸ“„ åŸå§‹æ•°æ®CSV: {original_csv}")
            print(f"  ğŸ§¹ æ¸…æ´—åCSV: {cleaned_csv}")
            print(f"  ğŸ“Š å¤„ç†æŠ¥å‘Š: {report_file}")
            
            print(f"\næ•°æ®ç»Ÿè®¡:")
            print(f"  åŸå§‹æ•°æ®: {self.raw_data.shape[0]:,} è¡Œ Ã— {self.raw_data.shape[1]} åˆ—")
            print(f"  æ¸…æ´—åæ•°æ®: {self.cleaned_data.shape[0]:,} è¡Œ Ã— {self.cleaned_data.shape[1]} åˆ—")
            print(f"  æ•°æ®ä¿ç•™ç‡: {len(self.cleaned_data) / len(self.raw_data) * 100:.2f}%")
            
            if summary_report["æ¸…æ´—æ•ˆæœ"]["åˆ é™¤çš„ç¼ºå¤±å€¼"] > 0:
                print(f"  å¤„ç†ç¼ºå¤±å€¼: {summary_report['æ¸…æ´—æ•ˆæœ']['åˆ é™¤çš„ç¼ºå¤±å€¼']:,} ä¸ª")
            if summary_report["æ¸…æ´—æ•ˆæœ"]["åˆ é™¤çš„é‡å¤è¡Œ"] > 0:
                print(f"  åˆ é™¤é‡å¤è¡Œ: {summary_report['æ¸…æ´—æ•ˆæœ']['åˆ é™¤çš„é‡å¤è¡Œ']:,} è¡Œ")
            
            print("="*60)
            
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä½¿ç”¨"""
    print("H5è½¬CSVå¤„ç†å™¨")
    print("="*50)
    
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = H5ToCSVProcessor()
    
    # ç¤ºä¾‹ï¼šå¤„ç†H5æ–‡ä»¶
    h5_files = [
        "ä¸­ä¿¡å»ºæŠ•/ä»»åŠ¡ä¸€/MinutesIdx.h5",  # å¦‚æœå­˜åœ¨çš„è¯
        # å¯ä»¥æ·»åŠ æ›´å¤šH5æ–‡ä»¶è·¯å¾„
    ]
    
    success_count = 0
    
    for h5_file in h5_files:
        if os.path.exists(h5_file):
            print(f"\næ­£åœ¨å¤„ç†: {h5_file}")
            if processor.process_h5_to_csv(h5_file, "converted_data"):
                success_count += 1
                print(f"âœ… {h5_file} å¤„ç†æˆåŠŸ")
            else:
                print(f"âŒ {h5_file} å¤„ç†å¤±è´¥")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {h5_file}")
    
    if success_count == 0:
        print("\næ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„H5æ–‡ä»¶")
        
       
if __name__ == "__main__":
    main()